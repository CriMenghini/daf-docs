Dataset
=======

The DAF framework is based on the abstraction (concept) of *dataset*: All
data stored in the DAF is organized into logical entities called
datasets.

A *dataset* is a combination of metadata, data, and other operational
information required to manage ingestion, update, lookup, creation of views and
alike. It is general purpose, in the sense that it can be used to model
both batch, streaming, and semi-structured data.

Types of datasets
----------------

We identify two main types of *datasets*:

-  **Standard datasets** describe phenomena that are
   common nationwide. These datasets are thus defined as datasets with national
   relevance and supported by the highest level of
   information/metadata. They follow a detailed set of rules 
   and standardization mechanisms that make them homogeneous across data sources (there may be multiple
   data sources describing the same phenomena, e.g., the bike sharing
   phenomena can be analyzed using data coming from Milan, Turin, Rome, etc...). 

-  **Ordinary datasets** have "owner" relevance, in the sense that they
   are defined and generated by a specific owner for its specific usage.
   They do not obey to a standard nationwide schema (data model), but the owner needs
   to specify metadata and information about the dataset before ingesting the
   data into the Big Data Platform of the DAF.


Dataset management phases
-------------------------

The dataset management is based on two main phases:

1. *Creation of a Dataset*: A user, registered to the Dataportal, can create (add) a dataset. The creation is guided through a specific web form where the user can load the data and provide a set of metadata describing the dataset. The data related to the registered dataset can be moved from the source towards the Big Data Platform. The platform provides APIs both for batch and streaming data ingestion. When data is received by the DAF platform, it is stored as raw data in the DAF data lake. Depending on the information provided by the dataset's owner during the registration phase, a dataset can be converted into standard big data serializazion formats (e.g., `Avro <https://avro.apache.org/>`_ and `Parquet <https://parquet.apache.org/>`_) and 'stored' into operational databases (e.g., HBase, Impala, etc).

2. *Dataset analysis* - once a dataset is available and properly stored
   in the datalake, the analysis phase can be enabled. The DAF
   platform provides several tools for processing and analysing
   datasets. In particular the current tools supported by DAF are: `Jupyter <../installation/docker/jupyter.html>`_, Metabase, Graphana, Superset.

3. *Dataset publication* - after the ingestion and standardization operations, the datasets can be published in the platform. Specifically, they can be published as open data when this is applicable: the datasets are available for the re-use by anyone through the public area of the Dataportal. The datasets can be published in the platform with specific restrictions on its usage in the case they cannot be consumed as open datasets.

