Dataset
=======

The DAF platform revolves around the concept of *dataset*. In fact all
data stored in the DAF are organized into logical entities called
dataset.

A *dataset* is a combination of metadata, data, and other operational
info needed to manage ingestion, update, lookup, creation of views and
alike. It is general purpose, in the sense that it can be used to model
both batch, streaming, semi-structured data.

Types of dataset
----------------

Dataset in the DAF platform are of three types:

-  **Standard datasets** are defined as datasets with national
   relevance, standardized across data sources (there may be multiple
   data sources describing the same phenomena, i.e. the bike sharing
   phenomena can have a data source from "Comune di Milano", another
   from "Comune di Torino", etc.) and supported by the highest level of
   information/metadata. They are aimed at describing phenomena that are
   common nationwide and therefore are based on a detailed set of rules
   and standardization mechanisms that will make them homogeneous.

-  **Ordinary datasets** have "owner" relevance, in the sense that they
   are defined and generated by a specific owner for its specific usage.
   They do not obey to a standard nationwide schema, but the owner needs
   to specify metadata and info about the dataset before ingesting the
   data into the platform.

-  **Raw datasets** are those with the lowest level of information: DAF
   works more as a storage layer with simple look up mechanism via rest
   API. They are mostly used to manage Open Data without stringent
   metadata information.

Dataset lifecycle
-----------------

The lifecycle of a dataset is based on three main steps:

1. | *Creation of a Dataset*
   | An authenticated user registers can add a dataset to the DAF
     platform filling the `dataset registration
     form <../../../docs-usr/adding-a-new-dataset>`__. Then data related
     to the registered dataset can be moved from the source towards the
     DAF. The DAF platform provides APIs both for batch and streaming
     data ingestion. When data are received by the DAF platform, they
     are stored as raw data in the DAF datalake. Depending on the
     information provided by the dataset's owner during the registration
     phase, a dataset can be converted into standard big data
     serializazion formats (eg. Avro and Parquet) and 'stored' into
     operational databases (eg. HBase, Impala, etc). For more
     information about the ingestion phase see the `Data
     Ingestion <../data-ingestion>`__ documentation.

2. *Dataset analysis* - once a dataset is available and properly saved
   in the datalake, the user can move to the analysis phase. The DAF
   platform provides several tools for the processing and analysing
   datasets. `Here <???>`__ a list of the available tools.

3. *Data and insights publishing* - ...
